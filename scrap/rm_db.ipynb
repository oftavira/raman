{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Workflow:\n",
    "\n",
    "This script aims to collect free/open acess information in a persistent way.\n",
    "It does not overload the server and waits a few seconds between each request.\n",
    "\n",
    "To start the script we check for the current progress of the database consolidation.\n",
    "Meaning that we need to know which are the current links to visit in the 'urls.picke' object.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the urls from the pickle file\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "# The base url of the website JRS\n",
    "\n",
    "base_url = 'https://analyticalsciencejournals.onlinelibrary.wiley.com'\n",
    "\n",
    "if os.path.exists('urls.pickle'):\n",
    "    with open('urls.pickle','rb') as f:\n",
    "        # adding verbose to specify the loading of the urls\n",
    "        print('Loading the urls from the pickle file')\n",
    "        urls = pickle.load(f)\n",
    "else:\n",
    "    urls = {base_url+'/loi/10974555/year/'+str(i):'none' for i in range(1973,2024)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 49\n"
     ]
    }
   ],
   "source": [
    "# We create a function to display the number of \n",
    "# values of type str in the urls dictionary \n",
    "\n",
    "def count_values(urls):\n",
    "    count = 0\n",
    "    count2 = 0\n",
    "    for key in urls.keys():\n",
    "        if urls[key] != 'none':\n",
    "            count += 1\n",
    "        else:\n",
    "            count2 += 1\n",
    "    print(count, count2)\n",
    "\n",
    "count_values(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "\n",
    "# We define the get_page and its chrono version\n",
    "\n",
    "def get_page(route):\n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    driver.get(route)\n",
    "    content = driver.page_source\n",
    "    driver.close()\n",
    "    return content\n",
    "\n",
    "def get_page_chrono(route):\n",
    "    start = time.time()\n",
    "    content = get_page(route)\n",
    "    end = time.time()\n",
    "    print('Time elapsed: {}'.format(end - start))\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Then we can pick a url that has not been scrapped yet\n",
    "# and define it as the target\n",
    "\n",
    "def get_random_key(urls):\n",
    "    key = random.choice(list(urls.keys()))\n",
    "    if urls[key] == 'none':\n",
    "        return key\n",
    "    else:\n",
    "        return get_random_key(urls)\n",
    "\n",
    "target = get_random_key(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 22.7828471660614\n"
     ]
    }
   ],
   "source": [
    "# Converting the page into a soup to parse it with bs4\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "to_soup = get_page_chrono(target)\n",
    "soup = BeautifulSoup(to_soup, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seting the value of the target in the urls dictionary\n",
    "\n",
    "urls[target] = soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dumping the urls dictionary into a pickle file\n",
    "\n",
    "with open('urls.pickle','wb') as f:\n",
    "    pickle.dump(urls,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the links to visit\n",
    "\n",
    "candidates = []\n",
    "\n",
    "for element in soup.findAll('li', attrs={'class':'card clearfix'}):\n",
    "    # Finding the url\n",
    "    candidates.append(base_url + element.find('a')['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://analyticalsciencejournals.onlinelibrary.wiley.com/toc/10974555/1998/29/12',\n",
       " 'https://analyticalsciencejournals.onlinelibrary.wiley.com/toc/10974555/1998/29/10-11',\n",
       " 'https://analyticalsciencejournals.onlinelibrary.wiley.com/toc/10974555/1998/29/9',\n",
       " 'https://analyticalsciencejournals.onlinelibrary.wiley.com/toc/10974555/1998/29/8',\n",
       " 'https://analyticalsciencejournals.onlinelibrary.wiley.com/toc/10974555/1998/29/7',\n",
       " 'https://analyticalsciencejournals.onlinelibrary.wiley.com/toc/10974555/1998/29/6',\n",
       " 'https://analyticalsciencejournals.onlinelibrary.wiley.com/toc/10974555/1998/29/5',\n",
       " 'https://analyticalsciencejournals.onlinelibrary.wiley.com/toc/10974555/1998/29/4',\n",
       " 'https://analyticalsciencejournals.onlinelibrary.wiley.com/toc/10974555/1998/29/3',\n",
       " 'https://analyticalsciencejournals.onlinelibrary.wiley.com/toc/10974555/1998/29/2',\n",
       " 'https://analyticalsciencejournals.onlinelibrary.wiley.com/toc/10974555/1998/29/1']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 49\n"
     ]
    }
   ],
   "source": [
    "# We create a function to display the number of \n",
    "# values of type str in the urls dictionary \n",
    "\n",
    "def count_values(urls):\n",
    "    count = 0\n",
    "    count2 = 0\n",
    "    for key in urls.keys():\n",
    "        if urls[key] != 'none':\n",
    "            count += 1\n",
    "        else:\n",
    "            count2 += 1\n",
    "    print(count, count2)\n",
    "\n",
    "count_values(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seting the value of the target in the urls dictionary\n",
    "\n",
    "urls[target] = soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_page_chrono' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/phantom/githubs/raman/scrap/rm_db.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/phantom/githubs/raman/scrap/rm_db.ipynb#Y100sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m to_soup \u001b[39m=\u001b[39m get_page_chrono(candidates[\u001b[39m0\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/phantom/githubs/raman/scrap/rm_db.ipynb#Y100sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m soup \u001b[39m=\u001b[39m BeautifulSoup(to_soup, \u001b[39m'\u001b[39m\u001b[39mhtml.parser\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_page_chrono' is not defined"
     ]
    }
   ],
   "source": [
    "to_soup = get_page_chrono(candidates[0])\n",
    "soup = BeautifulSoup(to_soup, 'html.parser')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lawsnpro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
